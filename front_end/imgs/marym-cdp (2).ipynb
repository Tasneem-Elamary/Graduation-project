{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h1><center>APTOS 2019 Blindness Detection</center></h1>\n<h2><center>Diabetic retinopathy - SHAP model explainability</center></h2>\n![](https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/APTOS%202019%20Blindness%20Detection/aux_img.png)\n\nIn this work, I'll train a baseline ResNet50, evaluate the model, and use SHAP model explainability technique to help us better understand our model's predictions, and how we could further improve its performance.\n\n#### About [SHAP](https://github.com/slundberg/shap) from its source:\n\n<img src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png\" width=\"400\">\n\n##### SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](https://github.com/slundberg/shap#citations) for details).","metadata":{}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport shap\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\nfrom keras.models import Model\nfrom keras import optimizers, applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n\n\nfrom tensorflow import keras\n\n# Set seeds to make the experiment more reproducible.\n#from tensorflow import set_random_seed\nfrom tensorflow.python.framework import random_seed\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    #set_random_seed(seed)\n    random_seed.set_seed(seed)\n\nseed = 0\nseed_everything(seed)\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-07-01T06:20:43.937381Z","iopub.execute_input":"2023-07-01T06:20:43.937799Z","iopub.status.idle":"2023-07-01T06:20:43.957798Z","shell.execute_reply.started":"2023-07-01T06:20:43.937765Z","shell.execute_reply":"2023-07-01T06:20:43.956652Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{"_kg_hide-output":true}},{"cell_type":"code","source":"train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nprint('Number of train samples: ', train.shape[0])\nprint('Number of test samples: ', test.shape[0])\n\n# Preprocecss data\ntrain[\"id_code\"] = train[\"id_code\"].apply(lambda x: x + \".png\")\ntest[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\ntrain['diagnosis'] = train['diagnosis'].astype('str')\ndisplay(train.head())","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-07-01T06:20:43.960978Z","iopub.execute_input":"2023-07-01T06:20:43.961403Z","iopub.status.idle":"2023-07-01T06:20:44.001973Z","shell.execute_reply.started":"2023-07-01T06:20:43.961361Z","shell.execute_reply":"2023-07-01T06:20:44.000994Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Number of train samples:  3662\nNumber of test samples:  1928\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            id_code diagnosis\n0  000c1434d8d7.png         2\n1  001639a390f0.png         4\n2  0024cdab0c1e.png         1\n3  002c21358ce6.png         0\n4  005b95c28852.png         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c1434d8d7.png</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001639a390f0.png</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0024cdab0c1e.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002c21358ce6.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>005b95c28852.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model parameters","metadata":{}},{"cell_type":"code","source":"# Model parameters\nBATCH_SIZE = 8\nEPOCHS = 40\nWARMUP_EPOCHS = 2\nLEARNING_RATE = 1e-4\nWARMUP_LEARNING_RATE = 1e-3\nHEIGHT = 320\nWIDTH = 320\nCANAL = 3\nN_CLASSES = train['diagnosis'].nunique()\nES_PATIENCE = 5\nRLROP_PATIENCE = 3\nDECAY_DROP = 0.5\nepsilon = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:44.003311Z","iopub.execute_input":"2023-07-01T06:20:44.004791Z","iopub.status.idle":"2023-07-01T06:20:44.012429Z","shell.execute_reply.started":"2023-07-01T06:20:44.004755Z","shell.execute_reply":"2023-07-01T06:20:44.011453Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Train test split","metadata":{}},{"cell_type":"markdown","source":"**train_test_split** splits arrays or matrices into random train and test subsets. That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.","metadata":{}},{"cell_type":"code","source":"X_train, X_val = train_test_split(train, test_size=0.2, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:44.015864Z","iopub.execute_input":"2023-07-01T06:20:44.016832Z","iopub.status.idle":"2023-07-01T06:20:44.024946Z","shell.execute_reply.started":"2023-07-01T06:20:44.016798Z","shell.execute_reply":"2023-07-01T06:20:44.023949Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Data generator","metadata":{}},{"cell_type":"markdown","source":"**ImageDataGenerator** Generate batches of tensor image data with real-time data augmentation\n\nWe can make the use of ImageDataGenerator class by passing the appropriate parameters and passing the required input to it. How many images will be generated depends on the size of the batch and the input data set that contains a specific number of inputs? For example, if the size of the batch is defined as 10 and we pass 1000 images in the input of outset of data then the number of images that will generate in each and every iteration of the training will be 10.\n\n*  **rescaling factor**. Defaults to None. we multiply the data by the value provided.\n*  **Rotation**. Degree range for random rotations.\n*  **horizontal_flip** Boolean. Randomly flip inputs horizontally. \n*  **vertical_flip** Boolean. Randomly flip inputs vertically.\n\n* **categorical**: 2D numpy array of encoded labels. Supports multi-label output.\n\n* **Batch Size**: For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder.\n\n* **seed**: Optional random seed for shuffling\n","metadata":{}},{"cell_type":"code","source":"train_datagen=ImageDataGenerator(rescale=1./255, \n                                 rotation_range=360,\n                                 horizontal_flip=True,\n                                 vertical_flip=True)\n\ntrain_generator=train_datagen.flow_from_dataframe(\n    dataframe=X_train,\n    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    target_size=(HEIGHT, WIDTH),\n    seed=0)\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n\nvalid_generator=validation_datagen.flow_from_dataframe(\n    dataframe=X_val,\n    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    class_mode=\"categorical\", \n    batch_size=BATCH_SIZE,   \n    target_size=(HEIGHT, WIDTH),\n    seed=0)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(  \n        dataframe=test,\n        directory = \"../input/aptos2019-blindness-detection/test_images/\",\n        x_col=\"id_code\",\n        batch_size=1,\n        class_mode=None,\n        shuffle=False,\n        target_size=(HEIGHT, WIDTH),\n        seed=0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:44.026570Z","iopub.execute_input":"2023-07-01T06:20:44.027084Z","iopub.status.idle":"2023-07-01T06:20:46.867766Z","shell.execute_reply.started":"2023-07-01T06:20:44.027049Z","shell.execute_reply":"2023-07-01T06:20:46.866679Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Found 2929 validated image filenames belonging to 5 classes.\nFound 733 validated image filenames belonging to 5 classes.\nFound 1928 validated image filenames.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\ndef custom_loss(y_true, y_pred):\n    # compute class weights\n    \n\n    # convert integer labels to one-hot encoding\n    y_true = K.one_hot(K.cast(y_true, 'int32'), num_classes=5)\n\n    # compute cross-entropy loss\n    loss = K.categorical_crossentropy(y_true, y_pred)\n\n    # multiply loss by class weights\n    weights = K.gather(class_weight, K.argmax(y_true, axis=-1))\n    weights = K.cast(weights, 'float32')\n    loss = loss * weights\n\n    # compute mean loss\n    loss = K.mean(loss)\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:41:05.890065Z","iopub.execute_input":"2023-07-01T06:41:05.891048Z","iopub.status.idle":"2023-07-01T06:41:05.898531Z","shell.execute_reply.started":"2023-07-01T06:41:05.891016Z","shell.execute_reply":"2023-07-01T06:41:05.897478Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"**include_top**:  whether to include the fully-connected layer at the top of the network.\n> **weights**: \n* one of None (random initialization) \n* 'imagenet' (pre-training on ImageNet) \n* the path to the weights file to be loaded","metadata":{}},{"cell_type":"markdown","source":"# Train top layers","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ndef create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    #base_model = applications.ResNet50(weights=None, \n                                       #include_top=False,\n                                       #input_tensor=input_tensor)',\n    base_model= applications.VGG19(weights=None,\n                                include_top=False,\n                                input_tensor=input_tensor)\n    base_model.load_weights('/kaggle/input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)  #dropout as averaging together the results of many individual neural networks\n    x = Dense(2048, activation='relu')(x)   #It's depend more on number of classes.\n    x = Dropout(0.5)(x)\n    final_output = Dense(n_out, activation='softmax', name='final_output')(x) \n    #The softmax activation function transforms the raw outputs of the neural network into a vector of probabilities\n    \n    model = Model(input_tensor, final_output)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:41:11.221390Z","iopub.execute_input":"2023-07-01T06:41:11.221789Z","iopub.status.idle":"2023-07-01T06:41:11.230650Z","shell.execute_reply.started":"2023-07-01T06:41:11.221760Z","shell.execute_reply":"2023-07-01T06:41:11.229476Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nmodel = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n\nfor layer in model.layers:\n    layer.trainable = False\n# the amount of layers that you want to freeze (not train)\n# the good practice is from top to bottom. You should tune the number of frozen layers by yourself. \n# But take into account that the more unfrozen layers you have, the slower is training.\n\nfor i in range(-5, 0):\n    model.layers[i].trainable = True\n\nclasses = np.unique(train['diagnosis'].astype('int').values)\n\nclass_weight = class_weight.compute_class_weight('balanced', classes=np.unique(train['diagnosis'].astype('int').values), y=train['diagnosis'].astype('int').values)\n\n# and the class weights for the training set can be computed like this\n\nmetric_list = [\"accuracy\"]\noptimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=custom_loss,  metrics=metric_list)\n# categorical_crossentropy: Used as a loss function for multi-class classification model \n#where there are two or more output labels. The output label is assigned one-hot category encoding value in form of 0s and 1. \n#The output label, if present in integer form, is converted into categorical encoding using keras.utils to_categorical method.\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T06:41:30.076800Z","iopub.execute_input":"2023-07-01T06:41:30.077255Z","iopub.status.idle":"2023-07-01T06:41:32.158534Z","shell.execute_reply.started":"2023-07-01T06:41:30.077219Z","shell.execute_reply":"2023-07-01T06:41:32.156426Z"},"trusted":true},"execution_count":61,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m---> 15\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_class_weight\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues), y\u001b[38;5;241m=\u001b[39mtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# and the class weights for the training set can be computed like this\u001b[39;00m\n\u001b[1;32m     19\u001b[0m metric_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'compute_class_weight'"],"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'compute_class_weight'","output_type":"error"}]},{"cell_type":"code","source":"x, y = next(train_generator)\nprint(\"Shape of x:\", x.shape)\nprint(\"Shape of y:\", y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:32:45.226301Z","iopub.execute_input":"2023-07-01T06:32:45.226721Z","iopub.status.idle":"2023-07-01T06:32:47.429129Z","shell.execute_reply.started":"2023-07-01T06:32:45.226688Z","shell.execute_reply":"2023-07-01T06:32:47.427850Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Shape of x: (8, 320, 320, 3)\nShape of y: (8, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"x, y = next(train_generator)\nloss = custom_loss(y_true=y, y_pred=model.predict(x))\nprint(\"Shape of loss:\", loss.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:39:25.372997Z","iopub.execute_input":"2023-07-01T06:39:25.374122Z","iopub.status.idle":"2023-07-01T06:39:28.007984Z","shell.execute_reply.started":"2023-07-01T06:39:25.374083Z","shell.execute_reply":"2023-07-01T06:39:28.005540Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 313ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_generator)\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mshape)\n","Cell \u001b[0;32mIn[49], line 5\u001b[0m, in \u001b[0;36mcustom_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(y_true, y_pred):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# compute class weights\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 5\u001b[0m     class_weight \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weight\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mclasses, y\u001b[38;5;241m=\u001b[39mtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# convert integer labels to one-hot encoding\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mone_hot(K\u001b[38;5;241m.\u001b[39mcast(y_true, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'class_weight' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'class_weight' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n\nhistory_warmup = model.fit_generator(generator=train_generator,\n                                     steps_per_epoch=STEP_SIZE_TRAIN,\n                                     validation_data=valid_generator,\n                                     validation_steps=STEP_SIZE_VALID,\n                                     epochs=WARMUP_EPOCHS,\n                                     class_weight=class_weight,\n                                     verbose=1).history","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T06:28:52.266037Z","iopub.execute_input":"2023-07-01T06:28:52.267086Z","iopub.status.idle":"2023-07-01T06:28:53.613355Z","shell.execute_reply.started":"2023-07-01T06:28:52.267054Z","shell.execute_reply":"2023-07-01T06:28:53.610692Z"},"trusted":true},"execution_count":40,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m STEP_SIZE_TRAIN \u001b[38;5;241m=\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m      2\u001b[0m STEP_SIZE_VALID \u001b[38;5;241m=\u001b[39m valid_generator\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mvalid_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m----> 4\u001b[0m history_warmup \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEP_SIZE_TRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEP_SIZE_VALID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWARMUP_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2636\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m \n\u001b[1;32m   2626\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2634\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2635\u001b[0m )\n\u001b[0;32m-> 2636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1291\u001b[0m, in \u001b[0;36mDataHandler._configure_dataset_and_inferred_steps\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m x\n\u001b[1;32m   1290\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n\u001b[0;32m-> 1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n\u001b[1;32m   1292\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(_make_class_weight_map_fn(class_weight))\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_steps(steps_per_epoch, dataset)\n","\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"],"ename":"ValueError","evalue":"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()","output_type":"error"}]},{"cell_type":"markdown","source":"# Fine-tune the complete model","metadata":{}},{"cell_type":"code","source":"for layer in model.layers:\n    layer.trainable = True\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\nrlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n\ncallback_list = [es, rlrop]\noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=custom_loss,  metrics=metric_list)\nmodel.summary()","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.315492Z","iopub.status.idle":"2023-07-01T06:20:50.316777Z","shell.execute_reply.started":"2023-07-01T06:20:50.316508Z","shell.execute_reply":"2023-07-01T06:20:50.316532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_finetunning = model.fit_generator(generator=train_generator,\n                                          steps_per_epoch=STEP_SIZE_TRAIN,\n                                          validation_data=valid_generator,\n                                          validation_steps=STEP_SIZE_VALID,\n                                          epochs=EPOCHS,\n                                          callbacks=callback_list,\n                                          class_weight=class_weights,\n                                          verbose=1).history","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.318305Z","iopub.status.idle":"2023-07-01T06:20:50.318820Z","shell.execute_reply.started":"2023-07-01T06:20:50.318551Z","shell.execute_reply":"2023-07-01T06:20:50.318574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model loss graph ","metadata":{}},{"cell_type":"code","source":"history = {'loss': history_warmup['loss'] + history_finetunning['loss'], \n           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'], \n           'acc': history_warmup['acc'] + history_finetunning['acc'], \n           'val_acc': history_warmup['val_acc'] + history_finetunning['val_acc']}\n\nsns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n\nax1.plot(history['loss'], label='Train loss')\nax1.plot(history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history['acc'], label='Train accuracy')\nax2.plot(history['val_acc'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.320251Z","iopub.status.idle":"2023-07-01T06:20:50.321329Z","shell.execute_reply.started":"2023-07-01T06:20:50.321038Z","shell.execute_reply":"2023-07-01T06:20:50.321068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\n## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Create empty arays to keep the predictions and labels\nlastFullTrainPred = np.empty((0, N_CLASSES))\nlastFullTrainLabels = np.empty((0, N_CLASSES))\nlastFullValPred = np.empty((0, N_CLASSES))\nlastFullValLabels = np.empty((0, N_CLASSES))\n\n# Add train predictions and labels\nfor i in range(STEP_SIZE_TRAIN+1):\n    im, lbl = next(train_generator)\n    scores = model.predict(im, batch_size=train_generator.batch_size)\n    lastFullTrainPred = np.append(lastFullTrainPred, scores, axis=0)\n    lastFullTrainLabels = np.append(lastFullTrainLabels, lbl, axis=0)\n\n# Add validation predictions and labels\nfor i in range(STEP_SIZE_VALID+1):\n    im, lbl = next(valid_generator)\n    scores = model.predict(im, batch_size=valid_generator.batch_size)\n    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n    \n    \nlastFullComPred = np.concatenate((lastFullTrainPred, lastFullValPred))\nlastFullComLabels = np.concatenate((lastFullTrainLabels, lastFullValLabels))\ncomplete_labels = [np.argmax(label) for label in lastFullComLabels]\n\ntrain_preds = [np.argmax(pred) for pred in lastFullTrainPred]\ntrain_labels = [np.argmax(label) for label in lastFullTrainLabels]\nvalidation_preds = [np.argmax(pred) for pred in lastFullValPred]\nvalidation_labels = [np.argmax(label) for label in lastFullValLabels]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.322991Z","iopub.status.idle":"2023-07-01T06:20:50.323480Z","shell.execute_reply.started":"2023-07-01T06:20:50.323237Z","shell.execute_reply":"2023-07-01T06:20:50.323259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\nlabels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\ntrain_cnf_matrix = confusion_matrix(train_labels, train_preds)\nvalidation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n\ntrain_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\nvalidation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n\ntrain_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\nvalidation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n\nsns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=ax1).set_title('Train')\nsns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=sns.cubehelix_palette(8), ax=ax2).set_title('Validation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:50.325133Z","iopub.status.idle":"2023-07-01T06:20:50.325644Z","shell.execute_reply.started":"2023-07-01T06:20:50.325386Z","shell.execute_reply":"2023-07-01T06:20:50.325410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quadratic Weighted Kappa","metadata":{}},{"cell_type":"code","source":"print(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds,train_labels, weights='quadratic'))\nprint(\"Validation Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels, weights='quadratic'))\nprint(\"Complete set Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds+validation_preds, train_labels+validation_labels, weights='quadratic'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.327392Z","iopub.status.idle":"2023-07-01T06:20:50.327870Z","shell.execute_reply.started":"2023-07-01T06:20:50.327612Z","shell.execute_reply":"2023-07-01T06:20:50.327642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, cohen_kappa_score,accuracy_score\nprint(\"Train Accuracy: %.3f\" % accuracy_score(train_labels,train_preds))\nprint(\"Validation Accuracy: %.3f\" % accuracy_score(validation_labels, validation_preds))\nprint(\"Complete set Accuracy: %.3f\" % accuracy_score(train_labels+validation_labels,train_preds+validation_preds))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:50.329758Z","iopub.status.idle":"2023-07-01T06:20:50.330255Z","shell.execute_reply.started":"2023-07-01T06:20:50.330000Z","shell.execute_reply":"2023-07-01T06:20:50.330023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SHAP Model explainability\n\n#### About SHAP's DeepExplainer from the [source repository](https://github.com/slundberg/shap#deep-learning-example-with-deepexplainer-tensorflowkeras-models): \n- Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc.\n\n### First let's see the images that we will explain","metadata":{}},{"cell_type":"code","source":"n_explain = 2\nvalid_generator.batch_size = 10 # background dataset\nbackground, lbls = next(valid_generator)\n\nsns.set_style(\"white\")\nplt.figure(figsize=[8, 8])\nfor index, image in enumerate(background[:n_explain]):\n    plt.subplot(n_explain, 1, index+1)\n    plt.imshow(image)\n    plt.title(\"Image %s, Label: %s\" % (index, np.argmax(lbls[index])))\n    \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.332042Z","iopub.status.idle":"2023-07-01T06:20:50.332543Z","shell.execute_reply.started":"2023-07-01T06:20:50.332295Z","shell.execute_reply":"2023-07-01T06:20:50.332318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now the SHAP explanation","metadata":{}},{"cell_type":"code","source":"# explain predictions of the model on \"n_explain\" images\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(background)\n\n# plot the feature attributions\nshap.image_plot(shap_values, -background[:n_explain], labels=lbls, hspace=0.1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.334443Z","iopub.status.idle":"2023-07-01T06:20:50.334955Z","shell.execute_reply.started":"2023-07-01T06:20:50.334687Z","shell.execute_reply":"2023-07-01T06:20:50.334710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The plot above explains five outputs (our five levels of diabetic retinopathy 0-5) for three different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left (they are black because most of the pixels are greater than 0), and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset, here I'm using 10 images) and the current model output. \n- Note that for the images that the label is \"1.0\" (the correct one), we a greater pink area.\n- Labels that have as much pink area as the correct one are labels that our model probably doesn't have a high confidence prediction.","metadata":{}},{"cell_type":"markdown","source":"## Let's try on a few more images","metadata":{}},{"cell_type":"code","source":"n_explain = 3\nbackground, lbls = next(valid_generator)\n\nsns.set_style(\"white\")\nplt.figure(figsize=[12, 12])\nfor index, image in enumerate(background[:n_explain]):\n    plt.subplot(n_explain, 1, index+1)\n    plt.imshow(image)\n    plt.title(\"Image %s, Label: %s\" % (index, np.argmax(lbls[index])))\n    \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.336465Z","iopub.status.idle":"2023-07-01T06:20:50.337533Z","shell.execute_reply.started":"2023-07-01T06:20:50.337251Z","shell.execute_reply":"2023-07-01T06:20:50.337277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# explain predictions of the model on \"n_explain\" images\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(background)\n\n# plot the feature attributions\nshap.image_plot(shap_values, -background[:n_explain], labels=lbls, hspace=0.1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.339147Z","iopub.status.idle":"2023-07-01T06:20:50.339642Z","shell.execute_reply.started":"2023-07-01T06:20:50.339379Z","shell.execute_reply":"2023-07-01T06:20:50.339402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply model to test set and output predictions","metadata":{}},{"cell_type":"code","source":"test_generator.reset()\nSTEP_SIZE_TEST = test_generator.n//test_generator.batch_size\npreds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)\npredictions = [np.argmax(pred) for pred in preds]\n\nfilenames = test_generator.filenames\nresults = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})\nresults['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.341333Z","iopub.status.idle":"2023-07-01T06:20:50.341844Z","shell.execute_reply.started":"2023-07-01T06:20:50.341562Z","shell.execute_reply":"2023-07-01T06:20:50.341583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions class distribution","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(1, 1, sharex='col', figsize=(24, 8.7))\nsns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\")\nsns.despine()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.343314Z","iopub.status.idle":"2023-07-01T06:20:50.344163Z","shell.execute_reply.started":"2023-07-01T06:20:50.343893Z","shell.execute_reply":"2023-07-01T06:20:50.343918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_csv('submission.csv', index=False)\nresults.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T06:20:50.345907Z","iopub.status.idle":"2023-07-01T06:20:50.346375Z","shell.execute_reply.started":"2023-07-01T06:20:50.346129Z","shell.execute_reply":"2023-07-01T06:20:50.346150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('vgg.h5')","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:50.348236Z","iopub.status.idle":"2023-07-01T06:20:50.348755Z","shell.execute_reply.started":"2023-07-01T06:20:50.348482Z","shell.execute_reply":"2023-07-01T06:20:50.348507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds,train_labels))\nprint(\"Validation Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:50.350260Z","iopub.status.idle":"2023-07-01T06:20:50.351130Z","shell.execute_reply.started":"2023-07-01T06:20:50.350889Z","shell.execute_reply":"2023-07-01T06:20:50.350914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle kernels output maryammahmoud/diabetic-retinopathy-shap-model-explainability -p /path/to/dest","metadata":{"execution":{"iopub.status.busy":"2023-07-01T06:20:50.352791Z","iopub.status.idle":"2023-07-01T06:20:50.353291Z","shell.execute_reply.started":"2023-07-01T06:20:50.353023Z","shell.execute_reply":"2023-07-01T06:20:50.353046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"kaggle kernels output maryammahmoud/diabetic-retinopathy-shap-model-explainability -p /path/to/dest","metadata":{}},{"cell_type":"markdown","source":"<a href=\"/kaggle/working/accuracy90.h5\"> Download File </a>","metadata":{}}]}